---
layout: article
title: Comment initialiser les poids d'un réseau de neurones ?
header:
  theme: dark
  background: 'linear-gradient(135deg, rgb(34, 139, 87), rgb(139, 34, 139))'
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .4), rgba(139, 34, 139, .4))'
    src: /docs/assets/images/cover3.jpg
mathjax: true
mathjax_autoNumber: false
---

Mon gradient fait des siennes !


# Vanishing  et Exploding Gradients

Si vous avez déjà travaillé avec les réseaux de neurones, vous devez être familiés avec les termes de "Vanishing Gradients" ou "Exploding Gradients".

Si ce n'est pas le cas, ce n'est pas grave ! Nous allons expliquer ces notions dans la suite.

Nous savons qu'un réseau de neurones est entrainé par la rétropropagation du gradient (backpropagation).
Au cours de l'entraînnement, les poids du réseau sont mis à jour de manière incrémentale :

$$
W \quad \longleftarrow \quad W-\lambda \frac{\partial \mathcal{L}}{\partial W}
$$

Nous voyons deux problèmes :

$$
\left|\frac{\partial \mathcal{L}}{\partial W}\right| \approx 0
$$

Les paramètres du réseau ne sont plus mis à jour, le réseau n'apprend plus.

$$
\left|\frac{\partial \mathcal{L}}{\partial W}\right| \approx  \infty
$$

Les paramètres du réseau sont mis à jour de manière trop brutale, l'apprentissage du réseau est mauvais.

## Explication en équations

Considérons un réseau de neurones de c couches.

Les couche cahées (hidden layer) dépendent des couches précédentes : 

$$
\mathbf{h}^{t+1}=f_{t}\left(\mathbf{h}^{t}\right)
$$

La sortie du réseau de neurone peut s'écrire :

$$
\mathbf{o}=f_{d} \circ \ldots \circ f_{1}(\mathbf{x})
$$


Intéressons-nous maintenant au calcul du gradient de la sortie :

$$
\partial_{\mathbf{W}_{t}} \mathbf{o}=\partial_{\mathbf{h}^{d-1}} \mathbf{h}^{d} \cdot \ldots \cdot \partial_{\mathbf{h}^{t+1}} \partial_{\mathbf{W}_{t}} \mathbf{h}^{t}
$$

## Fonctions d'activation



Dans les premiers réseaux de neurones, la fonction sigmoïde était souvent utilisée. Nous allons écrire deux petites fonctions python pour voir la tête de cette fonction et celle de sa dérivée.



```python
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import math


def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1+math.exp(-item)))
    return a

def ds(x):
    a = []
    for item in x:
        b=1/(1+math.exp(-item))
        a.append(b*(1-b))
    return a

x = np.arange(-10., 10., 0.2)
sig = sigmoid(x)
plt.plot(x,sig)
dsig=ds(x)
plt.plot(x,dsig)
plt.show()
```

Le résultat est le suivant :

<img class="image image--md" src="/assets/images/2019-05-31-init-sigmoide.png"/>



