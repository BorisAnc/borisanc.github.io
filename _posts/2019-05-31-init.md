---
layout: article
title: Comment initialiser les poids d'un réseau de neurones ?
header:
  theme: dark
  background: 'linear-gradient(135deg, rgb(34, 139, 87), rgb(139, 34, 139))'
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .4), rgba(139, 34, 139, .4))'
    src: /docs/assets/images/cover3.jpg
---

$$
\begin{align} \boldsymbol{y_l} = W_l \boldsymbol{x_l} \boldsymbol{b_l} \label{eq:fwd} \end{align}
$$

When $$a \ne 0$$, there are two solutions to $$ax^2 + bx + c = 0$$ and they are
$$x_1 = {-b + \sqrt{b^2-4ac} \over 2a}$$
$$x_2 = {-b - \sqrt{b^2-4ac} \over 2a} \notag$$


# Vanishing  et Exploding Gradients

Si vous avez déjà travaillé avec les réseaux de neurones, vous devez être familiés avec les termes de "Vanishing Gradients" ou "Exploding Gradients".

Si ce n'est pas le cas, ce n'est pas grave ! Nous allons expliquer ces notions dans la suite.

Nous savons qu'un réseau de neurones est entrainé par la rétropropagation du gradient (backpropagation).
Au cours de l'entraînnement, les poids du réseau sont mis à jour de manière incrémentale :
$$
W \quad \longleftarrow \quad W-\lambda \frac{\partial \mathcal{L}}{\partial W}
$$
Nous voyons deux problèmes :
$$
\left|\frac{\partial \mathcal{L}}{\partial W}\right| \approx 0
$$
Les paramètres du réseau ne sont plus mis à jour, le réseau n'apprend plus.
$$
\left|\frac{\partial \mathcal{L}}{\partial W}\right| \approx  \infty
$$
Les paramètres du réseau sont mis à jour de manière trop brutale, l'apprentissage du réseau est mauvais.

## Explication en équations

Considérons un réseau de neurones de c couches.

Les couche cahées (hidden layer) dépendent des couches précédentes : 
$$
\mathbf{h}^{t+1}=f_{t}\left(\mathbf{h}^{t}\right)
$$
La sortie du réseau de neurone peut s'écrire :
$$
\mathbf{o}=f_{d} \circ \ldots \circ f_{1}(\mathbf{x})
$$


Intéressons-nous maintenant au calcul du gradient de la sortie :
$$
\partial_{\mathbf{W}_{t}} \mathbf{o}=\partial_{\mathbf{h}^{d-1}} \mathbf{h}^{d} \cdot \ldots \cdot \partial_{\mathbf{h}^{t+1}} \partial_{\mathbf{W}_{t}} \mathbf{h}^{t}
$$
